{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "#import logging\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from urllib.parse import quote\n",
    "from typing import List\n",
    "\n",
    "import requests\n",
    "import yaml\n",
    "from dateutil import parser\n",
    "import jsonschema\n",
    "from jsonschema import validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = r\"/Users/srappel/Documents/github/GeoDiscovery-Utils/opendataharvest/config.yaml\"\n",
    "\n",
    "try:\n",
    "    with open(config_file, \"r\", encoding='utf-8') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Config file {config_file} not found\")\n",
    "    sys.exit()\n",
    "\n",
    "try:\n",
    "    CONFIG = config.get(\"CONFIG\")\n",
    "    OUTPUTDIR = Path(\"/Users/srappel/Documents/github/GeoDiscovery-Utils/opendataharvest/output_md/\")\n",
    "    LOGDIR = Path(CONFIG.get(\"LOGDIR\"))\n",
    "    DEFAULTBBOX = Path(CONFIG.get(\"DEFAULTBBOX\"))\n",
    "    CATALOG_KEY = CONFIG.get(\"CATALOG\", \"TestSites\")\n",
    "    CATALOG = config.get(CATALOG_KEY, None)\n",
    "    MAXRETRY = CONFIG.get(\"MAXRETRY\", 5)\n",
    "    SLEEPTIME = CONFIG.get(\"SLEEPTIME\", 1)\n",
    "\n",
    "    # Default Values\n",
    "    default_config = config.get(\"DEFAULT\", {})\n",
    "    MEMBEROF = default_config.get(\"MEMBEROF\", [])\n",
    "    RESOURCECLASS = default_config.get(\"RESOURCECLASS\", [])\n",
    "    ACCESSRIGHTS = default_config.get(\"ACCESSRIGHTS\")  # This is a single string value\n",
    "    MDVERSION = default_config.get(\"MDVERSION\")  # This is a single string value\n",
    "    LANG = default_config.get(\"LANG\", [])\n",
    "    PROVIDER = default_config.get(\"PROVIDER\")  # This is a single string value\n",
    "    SUPPRESSED = default_config.get(\"SUPPRESSED\")  # This is a boolean value\n",
    "    RIGHTS = default_config.get(\"RIGHTS\", [])\n",
    "    RESOURCETYPE = default_config.get(\"RESOURCETYPE\", [])\n",
    "    FORMAT = default_config.get(\"FORMAT\")\n",
    "    DESCRIPTION = default_config.get(\"DESCRIPTION\")\n",
    "\n",
    "    ## Get the JSON schema:\n",
    "    SCHEMA = CONFIG.get(\"SCHEMA\")\n",
    "\n",
    "except AttributeError as e:\n",
    "    print(f\"Unable to read all configuration values from {config_file}\")\n",
    "    print(e)\n",
    "    sys.exit()\n",
    "\n",
    "dt = str(datetime.now().strftime(r\"%Y%m%d%H%M%S\"))\n",
    "logfile_name = f\"_{dt}.log\"\n",
    "LOGFILE = LOGDIR / logfile_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Site:\n",
    "    \"\"\"\n",
    "    A class to represent a Site.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    site_name : str\n",
    "        The name of the site.\n",
    "    site_details : dict\n",
    "        The details of the site.\n",
    "    site_json : dict\n",
    "        The JSON data of the site.\n",
    "    site_skiplist : set\n",
    "        The set of UUIDs to skip.\n",
    "    site_applist : set\n",
    "        The set of UUIDs for applications.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __getitem__(self, key):\n",
    "        Gets the attribute of the object using the key.\n",
    "    __setitem__(self, key, value):\n",
    "        Sets the attribute of the object using the key and value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        site_name: str,\n",
    "        site_details: dict,\n",
    "        site_json: dict,\n",
    "        site_skiplist: list,\n",
    "        site_applist: list,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the Site object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            site_name : str\n",
    "                The name of the site.\n",
    "            site_details : dict\n",
    "                The details of the site.\n",
    "            site_json : dict\n",
    "                The JSON data of the site.\n",
    "            site_skiplist : list\n",
    "                The list of UUIDs to skip.\n",
    "            site_applist : list\n",
    "                The list of UUIDs for applications.\n",
    "        \"\"\"\n",
    "        self.site_name = site_name\n",
    "        self.site_details = site_details\n",
    "        self.site_json = site_json\n",
    "        self.site_skiplist = set(site_skiplist)\n",
    "        self.site_applist = set(site_applist)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Gets the attribute of the object using the key.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            key : str\n",
    "                The key to the attribute.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            The value of the attribute.\n",
    "        \"\"\"\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\"\n",
    "        Sets the attribute of the object using the key and value.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            key : str\n",
    "                The key to the attribute.\n",
    "            value : str\n",
    "                The value to set the attribute to.\n",
    "        \"\"\"\n",
    "        setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_site_data(site: str, details: dict) -> dict:\n",
    "    \"\"\"Fetch the site data with retries.\"\"\"\n",
    "    for i in range(MAXRETRY):\n",
    "        try:\n",
    "            response = requests.get(details[\"SiteURL\"], timeout=3)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"The content from {site} is not a valid JSON document.\")\n",
    "            return None\n",
    "        except (requests.HTTPError, requests.exceptions.Timeout) as e:\n",
    "            print(\n",
    "                f\"Received bad response from {site}. Retrying after {SLEEPTIME} seconds...\"\n",
    "            )\n",
    "            time.sleep(SLEEPTIME)\n",
    "            if i == (MAXRETRY - 1):\n",
    "                print(\n",
    "                    f\"Failed to connect to {site} after {MAXRETRY + 1} attempts.\"\n",
    "                )\n",
    "                print(str(e))\n",
    "                return None\n",
    "\n",
    "\n",
    "def get_uuid_list(details: dict, key: str) -> List[str]:\n",
    "    \"\"\"Extract UUIDs from details.\"\"\"\n",
    "    uuid_list = []\n",
    "    if key in details:\n",
    "        for item in details[key]:\n",
    "            uuid_list.append(item[\"UUID\"])\n",
    "    return uuid_list\n",
    "\n",
    "\n",
    "def harvest_sites() -> list:\n",
    "    \"\"\"Main function to harvest sites.\"\"\"\n",
    "    site_list = []\n",
    "    for site, details in CATALOG.items():\n",
    "        site_json = get_site_data(site, details)\n",
    "        if site_json is None:\n",
    "            continue\n",
    "        site_skiplist = get_uuid_list(details, \"SkipList\")\n",
    "        site_applist = get_uuid_list(details, \"AppList\")\n",
    "        current_Site = Site(\n",
    "            details[\"SiteName\"], details, site_json, site_skiplist, site_applist\n",
    "        )\n",
    "        site_list.append(current_Site)\n",
    "    return site_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AardvarkDataProcessor:\n",
    "    @staticmethod\n",
    "    def extract_data(dataset_dict):\n",
    "        # Extract data from dataset_dict\n",
    "        title = dataset_dict.get(\"title\", \"Untitled Dataset\")\n",
    "        identifier = dataset_dict[\"identifier\"]\n",
    "        description = re.sub(\"<[^<]+?>\", \"\", dataset_dict.get(\"description\", []))\n",
    "        creator = (\n",
    "            [dataset_dict[\"publisher\"][\"name\"]] if \"publisher\" in dataset_dict else []\n",
    "        )\n",
    "        issued = dataset_dict.get(\"issued\", \"\")\n",
    "        modified = dataset_dict.get(\"modified\", \"\")\n",
    "        keyword = dataset_dict.get(\"keyword\", [])\n",
    "        spatial = dataset_dict.get(\"spatial\", None)\n",
    "        distribution = dataset_dict.get(\"distribution\", None)\n",
    "        publisher = dataset_dict.get(\"publisher\", [])\n",
    "        landingPage = dataset_dict.get(\"landingPage\", \"\")\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"identifier\": identifier,\n",
    "            \"description\": description,\n",
    "            \"creator\": creator,\n",
    "            \"issued\": issued,\n",
    "            \"modified\": modified,\n",
    "            \"keyword\": keyword,\n",
    "            \"spatial\": spatial,\n",
    "            \"distribution\": distribution,\n",
    "            \"publisher\": publisher,\n",
    "            \"landingPage\": landingPage,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_id_sublayer(url):\n",
    "        id_pattern = r\"id=([a-zA-Z0-9]+)\"\n",
    "        sublayer_pattern = r\"sublayer=(\\d+)\"\n",
    "\n",
    "        id_match = re.search(id_pattern, url)\n",
    "        sublayer_match = re.search(sublayer_pattern, url)\n",
    "\n",
    "        id_value = id_match.group(1) if id_match else None\n",
    "        sublayer_value = sublayer_match.group(1) if sublayer_match else None\n",
    "\n",
    "        if id_value is None:\n",
    "            print(f\"No id was extracted from the url: {url}\")\n",
    "\n",
    "        return id_value, sublayer_value\n",
    "\n",
    "    @staticmethod\n",
    "    def process_dcat_spatial(spatial_string):\n",
    "        def is_in_range(value, range_min, range_max):\n",
    "            return range_min <= value <= range_max\n",
    "\n",
    "        # Extract coordinates\n",
    "        pattern = r\"(-?\\d+\\.\\d+)\"\n",
    "        matches = re.findall(pattern, spatial_string)\n",
    "\n",
    "        if len(matches) != 4:\n",
    "            raise ValueError(\"Non-conforming spatial bounding box\")\n",
    "\n",
    "        # Convert to floats and validate coordinates\n",
    "        coordinates = [float(coord) for coord in matches]\n",
    "        longitudes = coordinates[::2]\n",
    "        latitudes = coordinates[1::2]\n",
    "\n",
    "        if not all(is_in_range(lon, -180, 180) for lon in longitudes):\n",
    "            raise ValueError(\"Longitude coordinates must be between -180 and 180\")\n",
    "\n",
    "        if not all(is_in_range(lat, -90, 90) for lat in latitudes):\n",
    "            raise ValueError(\"Latitude coordinates must be between -90 and 90\")\n",
    "\n",
    "        # Ensure North is greater than South and East is greater than West\n",
    "        coordinates[1], coordinates[3] = sorted(latitudes, reverse=True)\n",
    "        coordinates[0], coordinates[2] = sorted(longitudes)\n",
    "\n",
    "        # Convert to ENVELOPE format\n",
    "        envelope = f\"ENVELOPE({coordinates[0]},{coordinates[2]},{coordinates[1]},{coordinates[3]})\"\n",
    "\n",
    "        return envelope\n",
    "\n",
    "    @staticmethod\n",
    "    def defaultBbox(website):\n",
    "        envelope = None\n",
    "        if \"DefaultBbox\" in website.site_details:\n",
    "            default_bbox = website.site_details[\"DefaultBbox\"]\n",
    "            with open(DEFAULTBBOX) as default_csv:\n",
    "                bboxreader = csv.DictReader(default_csv)\n",
    "                for row in bboxreader:\n",
    "                    if row[\"name\"] == default_bbox:\n",
    "                        envelope = f\"ENVELOPE({row['west']},{row['east']},{row['north']},{row['south']})\"\n",
    "        return envelope\n",
    "\n",
    "    @staticmethod\n",
    "    def getURL(distribution):\n",
    "        url = distribution.get(\"accessURL\", distribution.get(\"downloadURL\", \"invalid\"))\n",
    "        return quote(url, safe=\":/?=\")\n",
    "\n",
    "    @staticmethod\n",
    "    def process_distribution(distribution):\n",
    "        url = AardvarkDataProcessor.getURL(distribution)\n",
    "        if \"format\" not in distribution or url == \"invalid\":\n",
    "            return None\n",
    "\n",
    "        format_to_reference = {\n",
    "            \"ArcGIS GeoServices REST API\": {\n",
    "                \"FeatureServer\": \"urn:x-esri:serviceType:ArcGIS#FeatureLayer\",\n",
    "                \"ImageServer\": \"urn:x-esri:serviceType:ArcGIS#ImageMapLayer\",\n",
    "                \"MapServer\": \"urn:x-esri:serviceType:ArcGIS#DynamicMapLayer\",\n",
    "            },\n",
    "            \"ZIP\": \"http://schema.org/downloadUrl\",\n",
    "        }\n",
    "\n",
    "        format_references = format_to_reference.get(distribution[\"format\"], {})\n",
    "        if isinstance(format_references, dict):\n",
    "            for key, value in format_references.items():\n",
    "                if key in url:\n",
    "                    return {value: url}\n",
    "        else:\n",
    "            return {format_references: url}\n",
    "\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def format_fetcher(dataset_dict):\n",
    "        for distribution in dataset_dict[\"distribution\"]:\n",
    "            dct_format_s = FORMAT\n",
    "            gbl_resourceType_sm = RESOURCETYPE\n",
    "            gbl_resourceClass_sm = [\"Datasets\"]\n",
    "            if distribution[\"title\"] == \"Shapefile\":\n",
    "                dct_format_s = \"Shapefile\"\n",
    "            elif \"aerial\" in dataset_dict.get(\"title\", \"\").lower() or any(\n",
    "                keyword.lower() in [\"aerial photograph\", \"aerial imagery\"]\n",
    "                for keyword in dataset_dict.get(\"keyword\", [])\n",
    "            ):\n",
    "                gbl_resourceType_sm[0] = \"Aerial photographs\"\n",
    "                dct_format_s = \"Raster data\"\n",
    "                if \"Imagery\" not in gbl_resourceClass_sm:\n",
    "                    gbl_resourceClass_sm.append(\"Imagery\")\n",
    "\n",
    "        return dct_format_s, gbl_resourceType_sm, gbl_resourceClass_sm\n",
    "\n",
    "    @staticmethod\n",
    "    def issue_date_parser(dataset_dict):\n",
    "        dt_string = dataset_dict[\"issued\"]\n",
    "        try:\n",
    "            parsed_date = parser.parse(dt_string)\n",
    "            dct_issued_s = parsed_date.strftime(r'%Y-%m-%d')\n",
    "        except Exception as e:\n",
    "            print(f'Unable to parse the year from: \"{dt_string}\". Error: {e}')\n",
    "            dct_issued_s = dt_string\n",
    "\n",
    "        return dct_issued_s\n",
    "\n",
    "    @staticmethod\n",
    "    def load_schema():\n",
    "        response = requests.get(SCHEMA, timeout=3)\n",
    "        schema = json.loads(response.text)\n",
    "        return schema\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_json(json_data, schema):\n",
    "        try:\n",
    "            validate(instance=json_data, schema=schema)\n",
    "        except jsonschema.exceptions.ValidationError as err:\n",
    "            return False, err\n",
    "        return True, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aardvark:\n",
    "    \"\"\"\n",
    "    A class to represent a single dataset as an OGM Aardvark record\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_dict, website):\n",
    "        process_id_result = self._process_id(dataset_dict, website)\n",
    "        if not process_id_result: # Dataset is in the skiplist\n",
    "            return\n",
    "        self._initialize_default_field_values()\n",
    "        extracted_dataset_dict = AardvarkDataProcessor.extract_data(dataset_dict)\n",
    "        self._process_extracted_dataset_dict(extracted_dataset_dict, website)\n",
    "\n",
    "\n",
    "    def _initialize_default_field_values(self):\n",
    "        self.pcdm_memberOf_sm = MEMBEROF\n",
    "        self.gbl_resourceClass_sm = RESOURCECLASS\n",
    "        self.dct_accessRights_s = ACCESSRIGHTS\n",
    "        self.gbl_mdVersion_s = MDVERSION\n",
    "        self.dct_language_sm = LANG\n",
    "        self.schema_provider_s = PROVIDER\n",
    "        self.gbl_suppressed_b = SUPPRESSED\n",
    "        self.dct_rights_sm = RIGHTS\n",
    "\n",
    "    def _process_id(self, dataset_dict, website):\n",
    "        uuid, sublayer = AardvarkDataProcessor.extract_id_sublayer(\n",
    "            dataset_dict[\"identifier\"]\n",
    "        )\n",
    "        self.id = f\"{website.site_name}-{uuid}{sublayer if sublayer else ''}\"\n",
    "        self.uuid = uuid\n",
    "\n",
    "        if not self.id:\n",
    "            print(\"ID is required.\")\n",
    "            return None\n",
    "\n",
    "        # Stop processing if in skiplist\n",
    "        if self.uuid in website.site_skiplist:\n",
    "            print(f\"{self.uuid} is on the skiplist...\\n\")\n",
    "            return None\n",
    "\n",
    "        self.dct_identifier_sm = [dataset_dict[\"identifier\"]]\n",
    "        return True\n",
    "\n",
    "    def _process_extracted_dataset_dict(self, dataset_dict, website):\n",
    "        self.dct_spatial_sm = website.site_details[\"Spatial\"]\n",
    "\n",
    "        prefix = website.site_details[\"CreatedBy\"]\n",
    "        title = prefix + \" - \" + dataset_dict[\"title\"]\n",
    "        self.dct_title_s = title\n",
    "\n",
    "        self.gbl_mdModified_dt = datetime.now(timezone.utc).strftime(\n",
    "            \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "        )\n",
    "\n",
    "        self.dct_description_sm = [\n",
    "            re.sub(\"<[^<]+?>\", \"\", dataset_dict.get(\"description\", []))\n",
    "        ]\n",
    "        self.dct_description_sm.append(DESCRIPTION)\n",
    "\n",
    "        self.dct_creator_sm = (\n",
    "            [dataset_dict[\"publisher\"][\"name\"]] if \"publisher\" in dataset_dict else []\n",
    "        )\n",
    "\n",
    "        # dct_issued_s\n",
    "        self.dct_issued_s = AardvarkDataProcessor.issue_date_parser(dataset_dict)\n",
    "\n",
    "        self._process_spatial(dataset_dict, website)\n",
    "\n",
    "        # dcat_keyword_sm (string multiple!)\n",
    "        self.dcat_keyword_sm = dataset_dict[\"keyword\"]\n",
    "\n",
    "        self._process_distributions(dataset_dict)\n",
    "\n",
    "        self._process_temporal_coverage(dataset_dict)\n",
    "\n",
    "        # License and Rights\n",
    "        rights = self.dct_rights_sm\n",
    "        if dataset_dict.get(\"license\"):\n",
    "            rights.append(re.sub(\"<[^<]+?>\", \"\", dataset_dict.get(\"license\")))\n",
    "        self.dct_rights_sm = rights\n",
    "\n",
    "        # Format dct_format_s\n",
    "        def set_attributes_if_not_none(obj, attr_values):\n",
    "            attr_names = [\"dct_format_s\", \"gbl_resourceType_sm\", \"gbl_resourceClass_sm\"]\n",
    "            for attr_name, attr_value in zip(attr_names, attr_values):\n",
    "                if attr_value is not None:\n",
    "                    setattr(obj, attr_name, attr_value)\n",
    "\n",
    "        attr_values = AardvarkDataProcessor.format_fetcher(dataset_dict)\n",
    "        set_attributes_if_not_none(self, attr_values)\n",
    "\n",
    "        # Replace gbl_resourceClass_sm for web applications/websites\n",
    "        if self.uuid in website.site_applist:\n",
    "            self.gbl_resourceClass_sm = [\"Websites\"]\n",
    "\n",
    "    def _process_spatial(self, dataset_dict, website):\n",
    "        if \"spatial\" not in dataset_dict:\n",
    "            print(f\"No spatial information found for: {self.id}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            processed_spatial = AardvarkDataProcessor.process_dcat_spatial(\n",
    "                dataset_dict[\"spatial\"]\n",
    "            )\n",
    "            self.locn_geometry = self.dcat_bbox = processed_spatial\n",
    "        except ValueError as e:\n",
    "            print(\n",
    "                f\"There was a problem interpreting the bbox information for: {self.id}\\n\"\n",
    "                f\"\\t - at {dataset_dict['landingPage']}\\n\"\n",
    "                f\"\\t Error: {e}\\n\"\n",
    "            )\n",
    "            default_bbox = AardvarkDataProcessor.defaultBbox(website)\n",
    "            if default_bbox is not None:\n",
    "                self.locn_geometry = self.dcat_bbox = default_bbox\n",
    "                print(\"Using default envelope for the website.\\n\")\n",
    "            else:\n",
    "                print(f\"No default bounding box set for {website}\")\n",
    "\n",
    "    def _process_distributions(self, dataset_dict):\n",
    "        if \"distribution\" not in dataset_dict:\n",
    "            return\n",
    "\n",
    "        references = {\"http://schema.org/url\": dataset_dict[\"landingPage\"]}\n",
    "        for distribution in dataset_dict[\"distribution\"]:\n",
    "            reference = AardvarkDataProcessor.process_distribution(distribution)\n",
    "            if reference is not None:\n",
    "                references.update(reference)\n",
    "\n",
    "        self.dct_references_s = json.dumps(references).replace(\" \", \"\")\n",
    "\n",
    "    def _process_temporal_coverage(self, dataset_dict):\n",
    "        if \"modified\" in dataset_dict:\n",
    "            try:\n",
    "                index_date = parser.parse(dataset_dict[\"modified\"])\n",
    "                index_year = int(index_date.year)\n",
    "            except ImportError:\n",
    "                index_year = int(dataset_dict[\"modified\"][:4])\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "            self.gbl_indexYear_im = [index_year]\n",
    "            self.dct_temporal_sm = [f\"Modified {index_year}\"]\n",
    "        else:\n",
    "            self.gbl_indexYear_im = []\n",
    "\n",
    "        if \"issued\" in dataset_dict:\n",
    "            try:\n",
    "                index_date = parser.parse(dataset_dict[\"issued\"])\n",
    "                index_year = int(index_date.year)\n",
    "            except ImportError:\n",
    "                index_year = int(dataset_dict[\"issued\"][:4])\n",
    "            except Exception as e:\n",
    "                print(\"Problem processing the issued date.\")\n",
    "\n",
    "            self.gbl_indexYear_im.append(index_year)\n",
    "            if self.dct_temporal_sm:\n",
    "                self.dct_temporal_sm[0] = f\"Issued {index_year}\"\n",
    "            else:\n",
    "                self.dct_temporal_sm = [f\"Issued {index_year}\"]\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"\n",
    "        Serialize the object to a dictionary, excluding None or empty values.\n",
    "        \"\"\"\n",
    "        # List all the attributes that you want to include in the JSON output.\n",
    "        attributes = [\n",
    "            \"id\",\n",
    "            \"dct_title_s\",\n",
    "            \"dct_creator_sm\",\n",
    "            \"dct_identifier_sm\",\n",
    "            \"dct_rights_sm\",\n",
    "            \"gbl_resourceClass_sm\",\n",
    "            \"dct_accessRights_s\",\n",
    "            \"gbl_mdModified_dt\",\n",
    "            \"gbl_mdVersion_s\",\n",
    "            \"dct_language_sm\",\n",
    "            \"schema_provider_s\",\n",
    "            \"gbl_suppressed_b\",\n",
    "            \"dct_spatial_sm\",\n",
    "            \"dct_description_sm\",\n",
    "            \"dct_issued_s\",\n",
    "            \"dcat_keyword_sm\",\n",
    "            \"dct_references_s\",\n",
    "            \"dct_format_s\",\n",
    "            \"gbl_resourceType_sm\",\n",
    "            \"locn_geometry\",\n",
    "            \"dct_temporal_sm\",\n",
    "            \"gbl_indexYear_im\",\n",
    "        ]\n",
    "        # Build the dictionary with attribute names and their values if they are not None or empty.\n",
    "        return {\n",
    "            attr: getattr(self, attr)\n",
    "            for attr in attributes\n",
    "            if hasattr(self, attr) and getattr(self, attr)\n",
    "        }\n",
    "\n",
    "    def __str__(self):\n",
    "        # Use the to_dict method to get the dictionary representation of the object.\n",
    "        obj_dict = self.to_dict()\n",
    "        # Format the dictionary into a string for printing.\n",
    "        return \"\\n\".join(f\"{key}: {value}\" for key, value in obj_dict.items())\n",
    "\n",
    "    def toJSON(self):\n",
    "        aardvark_dict = self.to_dict()  # Use the new to_dict method\n",
    "        json_dump = json.dumps(aardvark_dict)\n",
    "        schema = AardvarkDataProcessor.load_schema()\n",
    "        is_valid, error = AardvarkDataProcessor.validate_json(aardvark_dict, schema)\n",
    "        if is_valid:\n",
    "            return json_dump\n",
    "        else:\n",
    "            print(f\"Failed JSON Validation:\\n{error}\")\n",
    "            return\n",
    "\n",
    "    def is_valid(self):\n",
    "        json_dump = self.toJSON()  # Call toJSON as a method\n",
    "        if json_dump is None:\n",
    "            return False, \"JSON serialization failed.\"\n",
    "\n",
    "        json_object = json.loads(\n",
    "            json_dump\n",
    "        )  # Parse the JSON string back into a dictionary\n",
    "        schema = AardvarkDataProcessor.load_schema()\n",
    "        is_valid, error = AardvarkDataProcessor.validate_json(json_object, schema)\n",
    "        if is_valid:\n",
    "            return True, None\n",
    "        else:\n",
    "            return False, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84c7b8d95af04cdda6b0c2ae26590531 is on the skiplist...\n",
      "\n",
      "bce9201dd312445b9b4567ee14d8032a is on the skiplist...\n",
      "\n",
      "d7f707071cd24b83ab3b9adb8a7d10ce is on the skiplist...\n",
      "\n",
      "7a1d3d055d4b4457845c721088c132f0 is on the skiplist...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m                         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(newfilePath)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not written...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 14\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m newfilePath \u001b[38;5;241m=\u001b[39m OUTPUTDIR \u001b[38;5;241m/\u001b[39m newfile\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(newfilePath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 14\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m \u001b[43mnew_aardvark_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoJSON\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(new_aardvark_object\u001b[38;5;241m.\u001b[39mtoJSON())\n",
      "Cell \u001b[0;32mIn[20], line 206\u001b[0m, in \u001b[0;36mAardvark.toJSON\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m aardvark_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()  \u001b[38;5;66;03m# Use the new to_dict method\u001b[39;00m\n\u001b[1;32m    205\u001b[0m json_dump \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(aardvark_dict)\n\u001b[0;32m--> 206\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mAardvarkDataProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m is_valid, error \u001b[38;5;241m=\u001b[39m AardvarkDataProcessor\u001b[38;5;241m.\u001b[39mvalidate_json(aardvark_dict, schema)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_valid:\n",
      "Cell \u001b[0;32mIn[19], line 156\u001b[0m, in \u001b[0;36mAardvarkDataProcessor.load_schema\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_schema\u001b[39m():\n\u001b[0;32m--> 156\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSCHEMA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     schema \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m schema\n",
      "File \u001b[0;32m~/Documents/github/GeoDiscovery-Utils/venv/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/GeoDiscovery-Utils/venv/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/GeoDiscovery-Utils/venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Documents/github/GeoDiscovery-Utils/venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Documents/github/GeoDiscovery-Utils/venv/lib/python3.12/site-packages/requests/adapters.py:589\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    586\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Documents/github/GeoDiscovery-Utils/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/github/GeoDiscovery-Utils/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/Documents/github/GeoDiscovery-Utils/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:1099\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m~/Documents/github/GeoDiscovery-Utils/venv/lib/python3.12/site-packages/urllib3/connection.py:616\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    615\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m    618\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/github/GeoDiscovery-Utils/venv/lib/python3.12/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/github/GeoDiscovery-Utils/venv/lib/python3.12/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main Function\n",
    "def main():\n",
    "    list_of_sites = harvest_sites()\n",
    "\n",
    "    for website in list_of_sites:\n",
    "        new_aardvark_objects = [\n",
    "            Aardvark(dataset, website) for dataset in website.site_json[\"dataset\"]\n",
    "        ]\n",
    "        for new_aardvark_object in new_aardvark_objects:\n",
    "            if new_aardvark_object.uuid not in website.site_skiplist:\n",
    "                newfile = f\"{new_aardvark_object.id}.json\"\n",
    "                newfilePath = OUTPUTDIR / newfile\n",
    "                with open(newfilePath, \"w\", encoding='utf-8') as f:\n",
    "                    json_data = new_aardvark_object.toJSON()\n",
    "                    if not json_data is None:\n",
    "                        f.write(new_aardvark_object.toJSON())\n",
    "                    else:\n",
    "                        print(f\"{str(newfilePath)} not written...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/srappel/Documents/github/GeoDiscovery-Utils/opendataharvest\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# get the current working directory\n",
    "current_working_directory = os.getcwd()\n",
    "\n",
    "# print output to the console\n",
    "print(current_working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
