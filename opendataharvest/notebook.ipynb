{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import yaml\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pprint import pp\n",
    "from urllib.parse import quote\n",
    "from dateutil import parser\n",
    "from geojson_rewind import rewind\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenDataSites = yaml.safe_load(open(r\"C:\\Users\\srappel\\Documents\\GitHub\\GeoDiscovery-Utils\\opendataharvest\\OpenDataSites.yaml\", \"r\"))\n",
    "OUTPUTDIR = Path(r'C:\\Users\\srappel\\Documents\\GitHub\\GeoDiscovery-Utils\\opendataharvest\\output_md')\n",
    "assert OUTPUTDIR.is_dir()\n",
    "\n",
    "CATALOG = OpenDataSites[\"ArcGIS_Sites\"]\n",
    "assert isinstance(CATALOG, dict)\n",
    "\n",
    "MAXRETRY = 4\n",
    "SLEEPTIME = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Site:\n",
    "    def __init__(self, site_name: str, site_details: dict, site_json: dict, site_skiplist: list):\n",
    "        self.site_name = site_name\n",
    "        self.site_details = site_details\n",
    "        self.site_json = site_json\n",
    "        self.site_skiplist = site_skiplist\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_sites() -> list:\n",
    "    site_list = [] # list of Site objects\n",
    "    for site, details in CATALOG.items():\n",
    "        for i in range(MAXRETRY):  # Retry up to 5 times\n",
    "            try:\n",
    "                response = requests.get(details[\"SiteURL\"], timeout=3)\n",
    "                response.raise_for_status()\n",
    "                site_json = response.json()\n",
    "                site_skiplist = []\n",
    "                if \"SkipList\" in details:\n",
    "                    for skip in details[\"SkipList\"]:\n",
    "                        site_skiplist.append(skip['UUID'])\n",
    "                current_Site = Site(details[\"SiteName\"], details, site_json, site_skiplist)\n",
    "                site_list.append(current_Site)\n",
    "                break  # If the request is successful, break the retry loop\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"The content from {site} is not a valid JSON document.\")\n",
    "                break  # If the content is not valid JSON, break the retry loop\n",
    "            except (requests.HTTPError, requests.exceptions.Timeout) as e:\n",
    "                print(f\"Received bad response from {site}. Retrying after {SLEEPTIME} seconds...\")\n",
    "                time.sleep(SLEEPTIME)  # Wait for 1 second before retrying\n",
    "                if i == (MAXRETRY - 1):  # If this was the last retry\n",
    "                    print(f\"Failed to connect to {site} after {MAXRETRY + 1} attempts.\")\n",
    "                    print(e)\n",
    "    return site_list\n",
    "\n",
    "list_of_sites = harvest_sites()\n",
    "\n",
    "for website in list_of_sites:\n",
    "    print(f'Website for {website.site_name}: {website.site_details[\"SiteURL\"]}')\n",
    "    print(f'Details for {website.site_name}:\\n{website.site_details}')\n",
    "    print(f'JSON for {website.site_name}:\\n{website.site_json}')\n",
    "    print(f'The following items are on the skiplist:{website.site_skiplist}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id_sublayer(url):\n",
    "    id_pattern = r'id=([a-zA-Z0-9]+)'\n",
    "    sublayer_pattern = r'sublayer=(\\d+)'\n",
    "\n",
    "    id_match = re.search(id_pattern, url)\n",
    "    sublayer_match = re.search(sublayer_pattern, url)\n",
    "\n",
    "    id_value = id_match.group(1) if id_match else None\n",
    "    sublayer_value = sublayer_match.group(1) if sublayer_match else None\n",
    "\n",
    "    return id_value, sublayer_value\n",
    "\n",
    "url = \"https://www.arcgis.com/home/item.html?id=0286131de8884484b26bbec4176ea403\"\n",
    "id_value, sublayer_value = extract_id_sublayer(url)\n",
    "\n",
    "print(f\"id: {id_value}, sublayer: {sublayer_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = list_of_sites[0].site_json[\"dataset\"][1]\n",
    "print(\"Aerial\" in dataset_dict.get('title'))\n",
    "pp(dataset_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open the link to the SiteURL in OpenRefine to get a easy to see tabular view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aardvark:\n",
    "    \n",
    "    def __init__(self, dataset_dict, website):\n",
    "        # Required fields\n",
    "        self.pcdm_memberOf_sm = [\"AGSLOpenDataHarvest\"]\n",
    "        self.gbl_resourceClass_sm = [\"Datasets\"]\n",
    "        self.dct_accessRights_s = \"public\"\n",
    "        self.gbl_mdVersion_s = \"Aardvark\"\n",
    "        self.dct_language_sm = [\"English\"]\n",
    "        self.schema_provider_s = \"American Geographical Society Library â€“ UWM Libraries\"\n",
    "        self.gbl_suppressed_b = False\n",
    "        self.dct_rights_sm = [\"Although this data is being distributed by the American Geographical Society Library at the University of Wisconsin-Milwaukee Libraries, no warranty expressed or implied is made by the University as to the accuracy of the data and related materials. The act of distribution shall not constitute any such warranty, and no responsibility is assumed by the University in the use of this data, or related materials.\"]\n",
    "        \n",
    "        assert \"title\" in dataset_dict and \"identifier\" in dataset_dict, \"Dataset missing title or identifier\"\n",
    "\n",
    "        # From YAML:\n",
    "        uuid, sublayer = extract_id_sublayer(dataset_dict['identifier'])\n",
    "        self.md_id = f\"{website.site_name}-{uuid}{sublayer if sublayer else ''}\"\n",
    "        self.uuid = uuid\n",
    "     \n",
    "        assert isinstance(self.md_id, str) and len(self.md_id) > 0, \"id is required\"\n",
    "\n",
    "        # dct_identifier_sm\n",
    "        self.dct_identifier_sm = dataset_dict[\"identifier\"]\n",
    "\n",
    "        # Process spatial bounding box\n",
    "        self.dct_spatial_sm = website.site_details['Spatial']\n",
    "        \n",
    "        # dct_title_s (REQUIRED)\n",
    "        self.dct_title_s = dataset_dict.get('title', '')\n",
    "        assert isinstance(self.dct_title_s, str) and len(self.dct_title_s) > 0, \"Title is required\"\n",
    "\n",
    "        # gbl_mdModified_dt (Required)\n",
    "        self.gbl_mdModified_dt = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        assert isinstance(self.gbl_mdModified_dt, str) and len(self.gbl_mdModified_dt) > 0, \"mdModified is required\"\n",
    "\n",
    "        # id (Required)\n",
    "        uuid, sublayer = extract_id_sublayer(dataset_dict[\"identifier\"])\n",
    "        self.md_id = website.site_name + \"-\" + uuid + (sublayer if not sublayer is None else \"\")      \n",
    "\n",
    "        # dct_description_sm\n",
    "        self.dct_description_sm = [re.sub('<[^<]+?>', '', dataset_dict.get('description', []))]\n",
    "        self.dct_description_sm.append(f\"This dataset was automatically cataloged from the author's Open Data Portal. In some cases, publication year and bounding coordinates shown here may be incorrect. Additional download formats may be available on the author's website. Please check the 'More details at' link for additional information.\")\n",
    "\n",
    "        # dct_creator_sm\n",
    "        self.dct_creator_sm = [dataset_dict['publisher']['name']] if 'publisher' in dataset_dict else []\n",
    "\n",
    "        # dct_issued_s\n",
    "        self.dct_issued_s = dataset_dict.get('issued', '')\n",
    "\n",
    "        # locn_geometry & dcat_bbox\n",
    "        def process_dcat_spatial(spatial_string):\n",
    "            # Extract coordinates\n",
    "            pattern = r\"(-?\\d+\\.\\d+)\"\n",
    "            matches = re.findall(pattern, spatial_string)\n",
    "\n",
    "            if len(matches) != 4:\n",
    "                raise ValueError(f\"Non-conforming spatial bounding box: {spatial_string}\")\n",
    "            \n",
    "            # Convert to floats\n",
    "            coordinates = [float(coord) for coord in matches]\n",
    "\n",
    "            # Create GeoJSON polygon\n",
    "            polygon = {\n",
    "                \"type\": \"Polygon\",\n",
    "                \"coordinates\": [[\n",
    "                    [coordinates[0], coordinates[1]],\n",
    "                    [coordinates[0], coordinates[3]],\n",
    "                    [coordinates[2], coordinates[3]],\n",
    "                    [coordinates[2], coordinates[1]],\n",
    "                    [coordinates[0], coordinates[1]]\n",
    "                ]]\n",
    "            }\n",
    "\n",
    "            # Ensure right hand rule\n",
    "            polygon = rewind(polygon)\n",
    "            \n",
    "            # Convert to ENVELOPE format\n",
    "            envelope = f\"ENVELOPE({coordinates[0]},{coordinates[2]},{coordinates[1]},{coordinates[3]})\"\n",
    "            \n",
    "            return envelope\n",
    "\n",
    "        if 'spatial' in dataset_dict:\n",
    "            try:\n",
    "                self.locn_geometry = self.dcat_bbox = process_dcat_spatial(dataset_dict[\"spatial\"])\n",
    "            except ValueError:\n",
    "                self.locn_geometry = self.dcat_bbox = None\n",
    "\n",
    "        # dcat_keyword_sm (string multiple!)\n",
    "        self.dcat_keyword_sm = dataset_dict.get('keyword', [])\n",
    "\n",
    "        # dct_references_s\n",
    "\n",
    "        def getURL(refs):\n",
    "            url = refs.get('accessURL', refs.get('downloadURL', 'invalid'))\n",
    "            return quote(url, safe=':/?=')\n",
    "\n",
    "        if 'distribution' in dataset_dict:\n",
    "            references = {\"http://schema.org/url\": dataset_dict[\"landingPage\"]}\n",
    "            for dist in dataset_dict['distribution']:\n",
    "                url = getURL(dist)\n",
    "                if 'format' in dist and url != \"invalid\":\n",
    "                    if dist['format'] == 'ArcGIS GeoServices REST API':\n",
    "                        if 'FeatureServer' in url:\n",
    "                            references['urn:x-esri:serviceType:ArcGIS#FeatureLayer'] = url\n",
    "                        elif 'ImageServer' in url:\n",
    "                            references['urn:x-esri:serviceType:ArcGIS#ImageMapLayer'] = url\n",
    "                        elif 'MapServer' in url:\n",
    "                            references['urn:x-esri:serviceType:ArcGIS#DynamicMapLayer'] = url\n",
    "                    elif dist['format'] == \"ZIP\":\n",
    "                        references['http://schema.org/downloadUrl'] = url\n",
    "            self.dct_references_s = json.dumps(references).replace(\" \",\"\")\n",
    "\n",
    "        # index year and temporal coverage\n",
    "        if 'modified' in dataset_dict:\n",
    "            try:\n",
    "                index_date = parser.parse(dataset_dict['modified'])\n",
    "                index_year = int(index_date.year)\n",
    "            except ImportError:\n",
    "                index_year = int(dataset_dict['modified'][:4])\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "            \n",
    "            self.gbl_indexYear_im = [index_year]\n",
    "            self.dct_temporal_sm = [f'Modified {index_year}']\n",
    "        else:\n",
    "            self.gbl_indexYear_im = []\n",
    "\n",
    "        if 'issued' in dataset_dict:\n",
    "            try:\n",
    "                index_date = parser.parse(dataset_dict['issued'])\n",
    "                index_year = int(index_date.year)\n",
    "            except ImportError:\n",
    "                index_year = int(dataset_dict['issued'][:4])\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "            self.gbl_indexYear_im.append(index_year)\n",
    "            if self.dct_temporal_sm:\n",
    "                self.dct_temporal_sm.append(f'Issued {index_year}')\n",
    "            else:\n",
    "                self.dct_temporal_sm = [f'Issued {index_year}']\n",
    "\n",
    "        # License and Rights\n",
    "        rights = self.dct_rights_sm\n",
    "        if dataset_dict.get(\"license\"):\n",
    "            rights.append(re.sub('<[^<]+?>', '', dataset_dict.get(\"license\")))\n",
    "        self.dct_rights_sm = rights\n",
    "\n",
    "        # Format dct_format_s\n",
    "        # TODO: this is only catching shapefile right now.\n",
    "        def format_fetcher():\n",
    "            for distribution in dataset_dict[\"distribution\"]:\n",
    "                if distribution['title'] == 'Shapefile':\n",
    "                    self.dct_format_s = \"Shapefile\"\n",
    "                    return\n",
    "                    \n",
    "            if (\"Aerial\" in dataset_dict.get('title') or \"aerial\" in dataset_dict.get('keyword') or \"imagery\" in dataset_dict.get('keyword')):\n",
    "                self.gbl_resourceType_sm = \"Aerial photographs\"\n",
    "                self.dct_format_s = \"Raster data\"\n",
    "                self.gbl_resourceClass_sm.append(\"Imagery\")\n",
    "                return\n",
    "\n",
    "        format_fetcher()\n",
    "         \n",
    "    def __str__(self):\n",
    "        return f\"\"\"\n",
    "        Title: {self.dct_title_s}\n",
    "        Id: {self.md_id}\n",
    "        Index Year: {self.gbl_indexYear_im}\n",
    "        Metadata Modified: {self.gbl_mdModified_dt}\n",
    "        Spatial: {self.dct_spatial_sm}\n",
    "        Description: {self.dct_description_sm}\n",
    "        Creator: {self.dct_creator_sm}\n",
    "        Issued: {self.dct_issued_s}\n",
    "        Spatial bbox: {self.locn_geometry}\n",
    "        References: {self.dct_references_s}\n",
    "        \"\"\"\n",
    "\n",
    "    def toJSON(self):\n",
    "        return json.dumps(vars(self))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for website in list_of_sites:\n",
    "    for dataset in website.site_json[\"dataset\"]:\n",
    "        new_aardvark_object = Aardvark(dataset, website)\n",
    "        if not new_aardvark_object.uuid in website.site_skiplist:\n",
    "            #print(new_aardvark_object.toJSON())\n",
    "            newfile = new_aardvark_object.md_id + \".json\"\n",
    "            newfilePath = OUTPUTDIR / newfile\n",
    "            f = open(newfilePath, \"w\")\n",
    "            print(f'Writing {newfilePath}')\n",
    "            f.write(new_aardvark_object.toJSON())\n",
    "            f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
