{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "#import logging\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from urllib.parse import quote\n",
    "from typing import List\n",
    "\n",
    "import requests\n",
    "import yaml\n",
    "from dateutil import parser\n",
    "import jsonschema\n",
    "from jsonschema import validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = r\"/Users/srappel/Documents/github/GeoDiscovery-Utils/opendataharvest/config.yaml\"\n",
    "\n",
    "try:\n",
    "    with open(config_file, \"r\", encoding='utf-8') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Config file {config_file} not found\")\n",
    "    sys.exit()\n",
    "\n",
    "try:\n",
    "    CONFIG = config.get(\"CONFIG\")\n",
    "    OUTPUTDIR = Path(\"/Users/srappel/Documents/github/GeoDiscovery-Utils/opendataharvest/output_md/\")\n",
    "    LOGDIR = Path(CONFIG.get(\"LOGDIR\"))\n",
    "    DEFAULTBBOX = Path(CONFIG.get(\"DEFAULTBBOX\"))\n",
    "    CATALOG_KEY = CONFIG.get(\"CATALOG\", \"TestSites\")\n",
    "    CATALOG = config.get(CATALOG_KEY, None)\n",
    "    MAXRETRY = CONFIG.get(\"MAXRETRY\", 5)\n",
    "    SLEEPTIME = CONFIG.get(\"SLEEPTIME\", 1)\n",
    "\n",
    "    # Default Values\n",
    "    default_config = config.get(\"DEFAULT\", {})\n",
    "    MEMBEROF = default_config.get(\"MEMBEROF\", [])\n",
    "    RESOURCECLASS = default_config.get(\"RESOURCECLASS\", [])\n",
    "    ACCESSRIGHTS = default_config.get(\"ACCESSRIGHTS\")  # This is a single string value\n",
    "    MDVERSION = default_config.get(\"MDVERSION\")  # This is a single string value\n",
    "    LANG = default_config.get(\"LANG\", [])\n",
    "    PROVIDER = default_config.get(\"PROVIDER\")  # This is a single string value\n",
    "    SUPPRESSED = default_config.get(\"SUPPRESSED\")  # This is a boolean value\n",
    "    RIGHTS = default_config.get(\"RIGHTS\", [])\n",
    "    RESOURCETYPE = default_config.get(\"RESOURCETYPE\", [])\n",
    "    FORMAT = default_config.get(\"FORMAT\")\n",
    "    DESCRIPTION = default_config.get(\"DESCRIPTION\")\n",
    "\n",
    "    ## Get the JSON schema:\n",
    "    SCHEMA = CONFIG.get(\"SCHEMA\")\n",
    "\n",
    "except AttributeError as e:\n",
    "    print(f\"Unable to read all configuration values from {config_file}\")\n",
    "    print(e)\n",
    "    sys.exit()\n",
    "\n",
    "dt = str(datetime.now().strftime(r\"%Y%m%d%H%M%S\"))\n",
    "logfile_name = f\"_{dt}.log\"\n",
    "LOGFILE = LOGDIR / logfile_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Site:\n",
    "    \"\"\"\n",
    "    A class to represent a Site.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    site_name : str\n",
    "        The name of the site.\n",
    "    site_details : dict\n",
    "        The details of the site.\n",
    "    site_json : dict\n",
    "        The JSON data of the site.\n",
    "    site_skiplist : set\n",
    "        The set of UUIDs to skip.\n",
    "    site_applist : set\n",
    "        The set of UUIDs for applications.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __getitem__(self, key):\n",
    "        Gets the attribute of the object using the key.\n",
    "    __setitem__(self, key, value):\n",
    "        Sets the attribute of the object using the key and value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        site_name: str,\n",
    "        site_details: dict,\n",
    "        site_json: dict,\n",
    "        site_skiplist: list,\n",
    "        site_applist: list,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the Site object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            site_name : str\n",
    "                The name of the site.\n",
    "            site_details : dict\n",
    "                The details of the site.\n",
    "            site_json : dict\n",
    "                The JSON data of the site.\n",
    "            site_skiplist : list\n",
    "                The list of UUIDs to skip.\n",
    "            site_applist : list\n",
    "                The list of UUIDs for applications.\n",
    "        \"\"\"\n",
    "        self.site_name = site_name\n",
    "        self.site_details = site_details\n",
    "        self.site_json = site_json\n",
    "        self.site_skiplist = set(site_skiplist)\n",
    "        self.site_applist = set(site_applist)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Gets the attribute of the object using the key.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            key : str\n",
    "                The key to the attribute.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            The value of the attribute.\n",
    "        \"\"\"\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\"\n",
    "        Sets the attribute of the object using the key and value.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            key : str\n",
    "                The key to the attribute.\n",
    "            value : str\n",
    "                The value to set the attribute to.\n",
    "        \"\"\"\n",
    "        setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_site_data(site: str, details: dict) -> dict:\n",
    "    \"\"\"Fetch the site data with retries.\"\"\"\n",
    "    for i in range(MAXRETRY):\n",
    "        try:\n",
    "            response = requests.get(details[\"SiteURL\"], timeout=3)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"The content from {site} is not a valid JSON document.\")\n",
    "            return None\n",
    "        except (requests.HTTPError, requests.exceptions.Timeout) as e:\n",
    "            print(\n",
    "                f\"Received bad response from {site}. Retrying after {SLEEPTIME} seconds...\"\n",
    "            )\n",
    "            time.sleep(SLEEPTIME)\n",
    "            if i == (MAXRETRY - 1):\n",
    "                print(\n",
    "                    f\"Failed to connect to {site} after {MAXRETRY + 1} attempts.\"\n",
    "                )\n",
    "                print(str(e))\n",
    "                return None\n",
    "\n",
    "\n",
    "def get_uuid_list(details: dict, key: str) -> List[str]:\n",
    "    \"\"\"Extract UUIDs from details.\"\"\"\n",
    "    uuid_list = []\n",
    "    if key in details:\n",
    "        for item in details[key]:\n",
    "            uuid_list.append(item[\"UUID\"])\n",
    "    return uuid_list\n",
    "\n",
    "\n",
    "def harvest_sites() -> list:\n",
    "    \"\"\"Main function to harvest sites.\"\"\"\n",
    "    site_list = []\n",
    "    for site, details in CATALOG.items():\n",
    "        site_json = get_site_data(site, details)\n",
    "        if site_json is None:\n",
    "            continue\n",
    "        site_skiplist = get_uuid_list(details, \"SkipList\")\n",
    "        site_applist = get_uuid_list(details, \"AppList\")\n",
    "        current_Site = Site(\n",
    "            details[\"SiteName\"], details, site_json, site_skiplist, site_applist\n",
    "        )\n",
    "        site_list.append(current_Site)\n",
    "    return site_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AardvarkDataProcessor:\n",
    "    @staticmethod\n",
    "    def extract_data(dataset_dict):\n",
    "        # Extract data from dataset_dict\n",
    "        title = dataset_dict.get(\"title\", \"Untitled Dataset\")\n",
    "        identifier = dataset_dict[\"identifier\"]\n",
    "        description = re.sub(\"<[^<]+?>\", \"\", dataset_dict.get(\"description\", []))\n",
    "        creator = (\n",
    "            [dataset_dict[\"publisher\"][\"name\"]] if \"publisher\" in dataset_dict else []\n",
    "        )\n",
    "        issued = dataset_dict.get(\"issued\", \"\")\n",
    "        modified = dataset_dict.get(\"modified\", \"\")\n",
    "        keyword = dataset_dict.get(\"keyword\", [])\n",
    "        spatial = dataset_dict.get(\"spatial\", None)\n",
    "        distribution = dataset_dict.get(\"distribution\", None)\n",
    "        publisher = dataset_dict.get(\"publisher\", [])\n",
    "        landingPage = dataset_dict.get(\"landingPage\", \"\")\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"identifier\": identifier,\n",
    "            \"description\": description,\n",
    "            \"creator\": creator,\n",
    "            \"issued\": issued,\n",
    "            \"modified\": modified,\n",
    "            \"keyword\": keyword,\n",
    "            \"spatial\": spatial,\n",
    "            \"distribution\": distribution,\n",
    "            \"publisher\": publisher,\n",
    "            \"landingPage\": landingPage,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_id_sublayer(url):\n",
    "        id_pattern = r\"id=([a-zA-Z0-9]+)\"\n",
    "        sublayer_pattern = r\"sublayer=(\\d+)\"\n",
    "\n",
    "        id_match = re.search(id_pattern, url)\n",
    "        sublayer_match = re.search(sublayer_pattern, url)\n",
    "\n",
    "        id_value = id_match.group(1) if id_match else None\n",
    "        sublayer_value = sublayer_match.group(1) if sublayer_match else None\n",
    "\n",
    "        if id_value is None:\n",
    "            print(f\"No id was extracted from the url: {url}\")\n",
    "\n",
    "        return id_value, sublayer_value\n",
    "\n",
    "    @staticmethod\n",
    "    def process_dcat_spatial(spatial_string):\n",
    "        def is_in_range(value, range_min, range_max):\n",
    "            return range_min <= value <= range_max\n",
    "\n",
    "        # Extract coordinates\n",
    "        pattern = r\"(-?\\d+\\.\\d+)\"\n",
    "        matches = re.findall(pattern, spatial_string)\n",
    "\n",
    "        if len(matches) != 4:\n",
    "            raise ValueError(\"Non-conforming spatial bounding box\")\n",
    "\n",
    "        # Convert to floats and validate coordinates\n",
    "        coordinates = [float(coord) for coord in matches]\n",
    "        longitudes = coordinates[::2]\n",
    "        latitudes = coordinates[1::2]\n",
    "\n",
    "        if not all(is_in_range(lon, -180, 180) for lon in longitudes):\n",
    "            raise ValueError(\"Longitude coordinates must be between -180 and 180\")\n",
    "\n",
    "        if not all(is_in_range(lat, -90, 90) for lat in latitudes):\n",
    "            raise ValueError(\"Latitude coordinates must be between -90 and 90\")\n",
    "\n",
    "        # Ensure North is greater than South and East is greater than West\n",
    "        coordinates[1], coordinates[3] = sorted(latitudes, reverse=True)\n",
    "        coordinates[0], coordinates[2] = sorted(longitudes)\n",
    "\n",
    "        # Convert to ENVELOPE format\n",
    "        envelope = f\"ENVELOPE({coordinates[0]},{coordinates[2]},{coordinates[1]},{coordinates[3]})\"\n",
    "\n",
    "        return envelope\n",
    "\n",
    "    @staticmethod\n",
    "    def defaultBbox(website):\n",
    "        envelope = None\n",
    "        if \"DefaultBbox\" in website.site_details:\n",
    "            default_bbox = website.site_details[\"DefaultBbox\"]\n",
    "            with open(DEFAULTBBOX) as default_csv:\n",
    "                bboxreader = csv.DictReader(default_csv)\n",
    "                for row in bboxreader:\n",
    "                    if row[\"name\"] == default_bbox:\n",
    "                        envelope = f\"ENVELOPE({row['west']},{row['east']},{row['north']},{row['south']})\"\n",
    "        return envelope\n",
    "\n",
    "    @staticmethod\n",
    "    def getURL(distribution):\n",
    "        url = distribution.get(\"accessURL\", None)\n",
    "        if url is None:\n",
    "            print(\"There is no accessURL, looking for downloadURL instead.\")\n",
    "            url = distribution.get(\"downloadURL\", None)\n",
    "        return quote(url, safe=\":/?=\")\n",
    "\n",
    "    @staticmethod\n",
    "    def process_distribution(distribution):\n",
    "        url = AardvarkDataProcessor.getURL(distribution)\n",
    "        if \"format\" not in distribution or url is None:\n",
    "            return None\n",
    "\n",
    "        format_to_reference = {\n",
    "            \"ArcGIS GeoServices REST API\": {\n",
    "                \"FeatureServer\": \"urn:x-esri:serviceType:ArcGIS#FeatureLayer\",\n",
    "                \"ImageServer\": \"urn:x-esri:serviceType:ArcGIS#ImageMapLayer\",\n",
    "                \"MapServer\": \"urn:x-esri:serviceType:ArcGIS#DynamicMapLayer\",\n",
    "            },\n",
    "            \"ZIP\": \"http://schema.org/downloadUrl\",\n",
    "        }\n",
    "\n",
    "        format_references = format_to_reference.get(distribution[\"format\"], {})\n",
    "        if isinstance(format_references, dict):\n",
    "            for key, value in format_references.items():\n",
    "                if key in url:\n",
    "                    return {value: url}\n",
    "        else:\n",
    "            return {format_references: url}\n",
    "\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def format_fetcher(dataset_dict):\n",
    "        for distribution in dataset_dict[\"distribution\"]:\n",
    "            dct_format_s = FORMAT\n",
    "            gbl_resourceType_sm = RESOURCETYPE\n",
    "            gbl_resourceClass_sm = [\"Datasets\"]\n",
    "            if distribution[\"title\"] == \"Shapefile\":\n",
    "                dct_format_s = \"Shapefile\"\n",
    "            elif \"aerial\" in dataset_dict.get(\"title\", \"\").lower() or any(\n",
    "                keyword.lower() in [\"aerial photograph\", \"aerial imagery\"]\n",
    "                for keyword in dataset_dict.get(\"keyword\", [])\n",
    "            ):\n",
    "                gbl_resourceType_sm[0] = \"Aerial photographs\"\n",
    "                dct_format_s = \"Raster data\"\n",
    "                if \"Imagery\" not in gbl_resourceClass_sm:\n",
    "                    gbl_resourceClass_sm.append(\"Imagery\")\n",
    "\n",
    "        return dct_format_s, gbl_resourceType_sm, gbl_resourceClass_sm\n",
    "\n",
    "    @staticmethod\n",
    "    def issue_date_parser(dataset_dict):\n",
    "        dt_string = dataset_dict[\"issued\"]\n",
    "        try:\n",
    "            parsed_date = parser.parse(dt_string)\n",
    "            dct_issued_s = parsed_date.strftime(r'%Y-%m-%d')\n",
    "        except Exception as e:\n",
    "            print(f'Unable to parse the year from: \"{dt_string}\". Error: {e}')\n",
    "            dct_issued_s = dt_string\n",
    "\n",
    "        return dct_issued_s\n",
    "\n",
    "    @staticmethod\n",
    "    def load_schema():\n",
    "        response = requests.get(SCHEMA, timeout=3)\n",
    "        schema = json.loads(response.text)\n",
    "        return schema\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_json(json_data, schema):\n",
    "        try:\n",
    "            validate(instance=json_data, schema=schema)\n",
    "        except jsonschema.exceptions.ValidationError as err:\n",
    "            return False, err\n",
    "        return True, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aardvark:\n",
    "    \"\"\"\n",
    "    A class to represent a single dataset as an OGM Aardvark record\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_dict, website):\n",
    "        process_id_result = self._process_id(dataset_dict, website)\n",
    "        if not process_id_result: # Dataset is in the skiplist\n",
    "            return\n",
    "        self._initialize_default_field_values()\n",
    "        extracted_dataset_dict = AardvarkDataProcessor.extract_data(dataset_dict)\n",
    "        self._process_extracted_dataset_dict(extracted_dataset_dict, website)\n",
    "\n",
    "\n",
    "    def _initialize_default_field_values(self):\n",
    "        self.pcdm_memberOf_sm = MEMBEROF\n",
    "        self.gbl_resourceClass_sm = RESOURCECLASS\n",
    "        self.dct_accessRights_s = ACCESSRIGHTS\n",
    "        self.gbl_mdVersion_s = MDVERSION\n",
    "        self.dct_language_sm = LANG\n",
    "        self.schema_provider_s = PROVIDER\n",
    "        self.gbl_suppressed_b = SUPPRESSED\n",
    "        self.dct_rights_sm = RIGHTS\n",
    "\n",
    "    def _process_id(self, dataset_dict, website):\n",
    "        uuid, sublayer = AardvarkDataProcessor.extract_id_sublayer(\n",
    "            dataset_dict[\"identifier\"]\n",
    "        )\n",
    "        self.id = f\"{website.site_name}-{uuid}{sublayer if sublayer else ''}\"\n",
    "        self.uuid = uuid\n",
    "\n",
    "        if not self.id:\n",
    "            print(\"ID is required.\")\n",
    "            return None\n",
    "\n",
    "        # Stop processing if in skiplist\n",
    "        if self.uuid in website.site_skiplist:\n",
    "            print(f\"{self.uuid} is on the skiplist...\\n\")\n",
    "            return None\n",
    "\n",
    "        self.dct_identifier_sm = [dataset_dict[\"identifier\"]]\n",
    "        return True\n",
    "\n",
    "    def _process_extracted_dataset_dict(self, dataset_dict, website):\n",
    "        self.dct_spatial_sm = website.site_details[\"Spatial\"]\n",
    "\n",
    "        prefix = website.site_details[\"CreatedBy\"]\n",
    "        title = prefix + \" - \" + dataset_dict[\"title\"]\n",
    "        self.dct_title_s = title\n",
    "\n",
    "        self.gbl_mdModified_dt = datetime.now(timezone.utc).strftime(\n",
    "            \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "        )\n",
    "\n",
    "        self.dct_description_sm = [\n",
    "            re.sub(\"<[^<]+?>\", \"\", dataset_dict.get(\"description\", []))\n",
    "        ]\n",
    "        self.dct_description_sm.append(DESCRIPTION)\n",
    "\n",
    "        self.dct_creator_sm = (\n",
    "            [dataset_dict[\"publisher\"][\"name\"]] if \"publisher\" in dataset_dict else []\n",
    "        )\n",
    "\n",
    "        # dct_issued_s\n",
    "        self.dct_issued_s = AardvarkDataProcessor.issue_date_parser(dataset_dict)\n",
    "\n",
    "        self._process_spatial(dataset_dict, website)\n",
    "\n",
    "        # dcat_keyword_sm (string multiple!)\n",
    "        self.dcat_keyword_sm = dataset_dict[\"keyword\"]\n",
    "\n",
    "        self._process_distributions(dataset_dict)\n",
    "\n",
    "        self._process_temporal_coverage(dataset_dict)\n",
    "\n",
    "        # License and Rights\n",
    "        rights = self.dct_rights_sm\n",
    "        if dataset_dict.get(\"license\"):\n",
    "            rights.append(re.sub(\"<[^<]+?>\", \"\", dataset_dict.get(\"license\")))\n",
    "        self.dct_rights_sm = rights\n",
    "\n",
    "        # Format dct_format_s\n",
    "        def set_attributes_if_not_none(obj, attr_values):\n",
    "            attr_names = [\"dct_format_s\", \"gbl_resourceType_sm\", \"gbl_resourceClass_sm\"]\n",
    "            for attr_name, attr_value in zip(attr_names, attr_values):\n",
    "                if attr_value is not None:\n",
    "                    setattr(obj, attr_name, attr_value)\n",
    "\n",
    "        attr_values = AardvarkDataProcessor.format_fetcher(dataset_dict)\n",
    "        set_attributes_if_not_none(self, attr_values)\n",
    "\n",
    "        # Replace gbl_resourceClass_sm for web applications/websites\n",
    "        if self.uuid in website.site_applist:\n",
    "            self.gbl_resourceClass_sm = [\"Websites\"]\n",
    "\n",
    "    def _process_spatial(self, dataset_dict, website):\n",
    "        if \"spatial\" not in dataset_dict:\n",
    "            print(f\"No spatial information found for: {self.id}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            processed_spatial = AardvarkDataProcessor.process_dcat_spatial(\n",
    "                dataset_dict[\"spatial\"]\n",
    "            )\n",
    "            self.locn_geometry = self.dcat_bbox = processed_spatial\n",
    "        except ValueError as e:\n",
    "            print(\n",
    "                f\"There was a problem interpreting the bbox information for: {self.id}\\n\"\n",
    "                f\"\\t - at {dataset_dict['landingPage']}\\n\"\n",
    "                f\"\\t Error: {e}\\n\"\n",
    "            )\n",
    "            default_bbox = AardvarkDataProcessor.defaultBbox(website)\n",
    "            if default_bbox is not None:\n",
    "                self.locn_geometry = self.dcat_bbox = default_bbox\n",
    "                print(\"Using default envelope for the website.\\n\")\n",
    "            else:\n",
    "                print(f\"No default bounding box set for {website}\")\n",
    "\n",
    "    def _process_distributions(self, dataset_dict):\n",
    "        if \"distribution\" not in dataset_dict:\n",
    "            return\n",
    "\n",
    "        references = {\"http://schema.org/url\": dataset_dict[\"landingPage\"]}\n",
    "        for distribution in dataset_dict[\"distribution\"]:\n",
    "            reference = AardvarkDataProcessor.process_distribution(distribution)\n",
    "            if reference is not None:\n",
    "                references.update(reference)\n",
    "\n",
    "        self.dct_references_s = json.dumps(references).replace(\" \", \"\")\n",
    "\n",
    "    def _process_temporal_coverage(self, dataset_dict):\n",
    "        if \"modified\" in dataset_dict:\n",
    "            try:\n",
    "                index_date = parser.parse(dataset_dict[\"modified\"])\n",
    "                index_year = int(index_date.year)\n",
    "            except ImportError:\n",
    "                index_year = int(dataset_dict[\"modified\"][:4])\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "            self.gbl_indexYear_im = [index_year]\n",
    "            self.dct_temporal_sm = [f\"Modified {index_year}\"]\n",
    "        else:\n",
    "            self.gbl_indexYear_im = []\n",
    "\n",
    "        if \"issued\" in dataset_dict:\n",
    "            try:\n",
    "                index_date = parser.parse(dataset_dict[\"issued\"])\n",
    "                index_year = int(index_date.year)\n",
    "            except ImportError:\n",
    "                index_year = int(dataset_dict[\"issued\"][:4])\n",
    "            except Exception as e:\n",
    "                print(\"Problem processing the issued date.\")\n",
    "\n",
    "            self.gbl_indexYear_im.append(index_year)\n",
    "            if self.dct_temporal_sm:\n",
    "                self.dct_temporal_sm[0] = f\"Issued {index_year}\"\n",
    "            else:\n",
    "                self.dct_temporal_sm = [f\"Issued {index_year}\"]\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"\n",
    "        Serialize the object to a dictionary, excluding None or empty values.\n",
    "        \"\"\"\n",
    "        # List all the attributes that you want to include in the JSON output.\n",
    "        attributes = [\n",
    "            \"id\",\n",
    "            \"dct_title_s\",\n",
    "            \"dct_creator_sm\",\n",
    "            \"dct_identifier_sm\",\n",
    "            \"dct_rights_sm\",\n",
    "            \"gbl_resourceClass_sm\",\n",
    "            \"dct_accessRights_s\",\n",
    "            \"gbl_mdModified_dt\",\n",
    "            \"gbl_mdVersion_s\",\n",
    "            \"dct_language_sm\",\n",
    "            \"schema_provider_s\",\n",
    "            \"gbl_suppressed_b\",\n",
    "            \"dct_spatial_sm\",\n",
    "            \"dct_description_sm\",\n",
    "            \"dct_issued_s\",\n",
    "            \"dcat_keyword_sm\",\n",
    "            \"dct_references_s\",\n",
    "            \"dct_format_s\",\n",
    "            \"gbl_resourceType_sm\",\n",
    "            \"locn_geometry\",\n",
    "            \"dct_temporal_sm\",\n",
    "            \"gbl_indexYear_im\",\n",
    "        ]\n",
    "        # Build the dictionary with attribute names and their values if they are not None or empty.\n",
    "        return {\n",
    "            attr: getattr(self, attr)\n",
    "            for attr in attributes\n",
    "            if hasattr(self, attr) and getattr(self, attr)\n",
    "        }\n",
    "\n",
    "    def __str__(self):\n",
    "        # Use the to_dict method to get the dictionary representation of the object.\n",
    "        obj_dict = self.to_dict()\n",
    "        # Format the dictionary into a string for printing.\n",
    "        return \"\\n\".join(f\"{key}: {value}\" for key, value in obj_dict.items())\n",
    "\n",
    "    def toJSON(self):\n",
    "        aardvark_dict = self.to_dict()  # Use the new to_dict method\n",
    "        json_dump = json.dumps(aardvark_dict)\n",
    "        schema = AardvarkDataProcessor.load_schema()\n",
    "        is_valid, error = AardvarkDataProcessor.validate_json(aardvark_dict, schema)\n",
    "        if is_valid:\n",
    "            return json_dump\n",
    "        else:\n",
    "            print(f\"Failed JSON Validation:\\n{error}\")\n",
    "            return\n",
    "\n",
    "    def is_valid(self):\n",
    "        json_dump = self.toJSON()  # Call toJSON as a method\n",
    "        if json_dump is None:\n",
    "            return False, \"JSON serialization failed.\"\n",
    "\n",
    "        json_object = json.loads(\n",
    "            json_dump\n",
    "        )  # Parse the JSON string back into a dictionary\n",
    "        schema = AardvarkDataProcessor.load_schema()\n",
    "        is_valid, error = AardvarkDataProcessor.validate_json(json_object, schema)\n",
    "        if is_valid:\n",
    "            return True, None\n",
    "        else:\n",
    "            return False, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Function\n",
    "def main():\n",
    "    list_of_sites = harvest_sites()\n",
    "\n",
    "    for website in list_of_sites:\n",
    "        new_aardvark_objects = [\n",
    "            Aardvark(dataset, website) for dataset in website.site_json[\"dataset\"]\n",
    "        ]\n",
    "        for new_aardvark_object in new_aardvark_objects:\n",
    "            if new_aardvark_object.uuid not in website.site_skiplist:\n",
    "                newfile = f\"{new_aardvark_object.id}.json\"\n",
    "                newfilePath = OUTPUTDIR / newfile\n",
    "                with open(newfilePath, \"w\", encoding='utf-8') as f:\n",
    "                    json_data = new_aardvark_object.toJSON()\n",
    "                    if not json_data is None:\n",
    "                        f.write(new_aardvark_object.toJSON())\n",
    "                    else:\n",
    "                        print(f\"{str(newfilePath)} not written...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add this:\n",
    "\n",
    "# In your existing project file\n",
    "from metadata_sorter import MetadataSorter\n",
    "\n",
    "# Initialize the sorter with your config file\n",
    "sorter = MetadataSorter(config_file='updated_config.yaml')\n",
    "\n",
    "# Example method that processes a single DCAT record\n",
    "def process_record(record):\n",
    "    classifications = sorter.format_fetcher(record)\n",
    "    print(f'Classifications: {classifications}')\n",
    "\n",
    "# Load your DCAT record (replace with your actual data loading logic)\n",
    "with open('dcat_record.json', 'r') as file:\n",
    "    record = json.load(file)\n",
    "\n",
    "# Process the record\n",
    "process_record(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
